{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbOq_H3HTi4I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.enc_conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
        "        self.enc_conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
        "        self.enc_conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.dec_conv1 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec_conv2 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec_conv3 = nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x = self.relu(self.enc_conv1(x))\n",
        "        x = self.relu(self.enc_conv2(x))\n",
        "        x = self.relu(self.enc_conv3(x))\n",
        "\n",
        "        # Decode\n",
        "        x = self.relu(self.dec_conv1(x))\n",
        "        x = self.relu(self.dec_conv2(x))\n",
        "        x = torch.sigmoid(self.dec_conv3(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHv7dNMveWuc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder layers\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            # nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            # nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 28 * 28, self.latent_dim),\n",
        "            nn.BatchNorm1d(self.latent_dim),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.enc_conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
        "        # self.enc_conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
        "        # self.enc_conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "\n",
        "        # # Latent space\n",
        "        # self.latent_dim = latent_dim\n",
        "        # self.fc1 = nn.Linear(64 * 28 * 28, self.latent_dim)\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim, 64 * 28 * 28),\n",
        "            nn.BatchNorm1d(64 * 28 * 28),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        # self.dec_conv1 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
        "        # self.dec_conv2 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)\n",
        "        # self.dec_conv3 = nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        # Classification layer\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.latent_dim, 1), nn.Sigmoid())\n",
        "\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     x = self.relu(self.enc_conv1(x))\n",
        "    #     x = self.relu(self.enc_conv2(x))\n",
        "    #     x = self.relu(self.enc_conv3(x))\n",
        "    #     x = x.view(x.size(0), -1)\n",
        "    #     x = self.relu(self.fc1(x))\n",
        "    #     return x\n",
        "\n",
        "    # def decode(self, z):\n",
        "    #     z = self.relu(self.fc2(z))\n",
        "    #     z = z.view(z.size(0), 64, 28, 28)\n",
        "    #     z = self.relu(self.dec_conv1(z))\n",
        "    #     z = self.relu(self.dec_conv2(z))\n",
        "    #     z = self.sigmoid(self.dec_conv3(z))\n",
        "    #     return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        z = self.enc(x)\n",
        "\n",
        "        # Classification\n",
        "        y = self.classifier(z)\n",
        "\n",
        "        # Decode\n",
        "        flat = self.fc2(z)\n",
        "        x_hat = flat.reshape(flat.shape[0], -1, 28, 28)\n",
        "        x_hat = self.dec(x_hat)\n",
        "\n",
        "        return x_hat, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhecTosoxkSC",
        "outputId": "f65bb285-4491-409e-d550-056c8527bdb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "893hG7kuF65G",
        "outputId": "33af6682-efc5-44f7-8ca7-48316c098cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4-aEElR-iQC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "import torchvision as v\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3qBWnjaE9KF"
      },
      "source": [
        "# dicomdataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwPToWiW-Zgp"
      },
      "outputs": [],
      "source": [
        "# # Define the path to the DICOM directory and the CSV file\n",
        "# dicom_dir = '/content/drive/MyDrive/DataWizardS/content/iaaa-data/DICOM'\n",
        "# csv_file = '/content/drive/MyDrive/DataWizardS/content/iaaa-data/labels.csv'\n",
        "# masks_dir = '/content/drive/MyDrive/DataWizardS/content/iaaa-data/masks'\n",
        "\n",
        "\n",
        "# # Define the batch size for the DataLoader\n",
        "# batch_size = 16\n",
        "\n",
        "# # Define the transforms to apply to the DICOM images\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(448),\n",
        "#     transforms.CenterCrop(448),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "# ])\n",
        "\n",
        "# # Define transforms\n",
        "# # transform = transforms.Compose([\n",
        "# #     transforms.Resize(224),\n",
        "# #     transforms.CenterCrop(224),\n",
        "# #     transforms.ToTensor(),\n",
        "# #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     # transforms.Normalize((0.5), (0.5))\n",
        "# # ])\n",
        "\n",
        "# # Load the CSV file into a pandas DataFrame\n",
        "# labels_df = pd.read_csv(csv_file)\n",
        "\n",
        "# labels_df['Label'] = labels_df['Label'].replace(['normal', 'abnormal'], [1, 0])\n",
        "\n",
        "# # Define a custom dataset class to load the DICOM images\n",
        "# class DICOMDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, dicom_dir, labels_df, masks_dir, transform=None, split=0.8):\n",
        "#         self.dicom_dir = dicom_dir\n",
        "#         self.labels_df = labels_df\n",
        "#         self.masks_dir = masks_dir\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.split = split\n",
        "#         self.train_len = int(len(self.labels_df) * self.split)\n",
        "#         self.test_len = len(self.labels_df) - self.train_len\n",
        "\n",
        "#         self.train_data = self.labels_df.iloc[:self.train_len]\n",
        "#         self.test_data = self.labels_df.iloc[self.train_len:]\n",
        "\n",
        "#         self.train = True\n",
        "\n",
        "#     def __len__(self):\n",
        "#         if self.train:\n",
        "#             return self.train_len\n",
        "#         else:\n",
        "#             return self.test_len\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if self.train:\n",
        "#             data = self.train_data\n",
        "#         else:\n",
        "#             data = self.test_data\n",
        "\n",
        "\n",
        "#         # Load the DICOM image\n",
        "#         subj_name = data.iloc[idx]['SOPInstanceUID']\n",
        "\n",
        "#         filename = subj_name + '.dcm'\n",
        "#         dicom_path = os.path.join(self.dicom_dir, filename)\n",
        "#         ds = pydicom.dcmread(dicom_path)\n",
        "\n",
        "#         # Get the label\n",
        "#         label = data.iloc[idx]['Label']\n",
        "\n",
        "#         # Convert to PIL image\n",
        "#         img_np = np.float64(ds.pixel_array)\n",
        "\n",
        "#         # applying mask on image numpy array before converting that to torch tensor\n",
        "#         if label == 'abnormal' :\n",
        "#             mask_name =  subj_name + '.png'\n",
        "#             mask_path = os.path.join(masks_dir, mask_name)\n",
        "#             mask = cv2.imread(mask_path, cv2.COLOR_BGR2GRAY)\n",
        "#             img_np = mask*img_np  # same as using cv2.bitwise_and()\n",
        "#             # img_np = cv2.bitwise_and(img_np, img_np, mask = mask)\n",
        "\n",
        "#         img = Image.fromarray(img_np).convert('L')\n",
        "\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "\n",
        "\n",
        "#         return img, label\n",
        "\n",
        "#     def set_train(self, train=True):\n",
        "#       self.train = train\n",
        "\n",
        "# dataset = DICOMDataset(dicom_dir, labels_df, masks_dir, transform=transform, split=0.8)\n",
        "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# dataset.set_train(False)\n",
        "# test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # labels_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDBI5t7BFH0_"
      },
      "source": [
        "# Dataset generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJVATf4OE2b1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the path to the DICOM directory and the CSV file\n",
        "dicom_dir = '/content/drive/MyDrive/DataWizardS2/iaaa-data/DICOM'\n",
        "csv_file = '/content/drive/MyDrive/DataWizardS2/iaaa-data/labels.csv'\n",
        "masks_dir = '/content/drive/MyDrive/DataWizardS2/iaaa-data/masks'\n",
        "\n",
        "\n",
        "# Define the batch size for the DataLoader\n",
        "batch_size = 8\n",
        "\n",
        "# Define the transforms to apply to the DICOM images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "labels_df = pd.read_csv(csv_file)\n",
        "labels_df['Label'] = labels_df['Label'].replace(['normal', 'abnormal'], [0, 1])\n",
        "\n",
        "# Define a custom dataset class to load the DICOM images\n",
        "class DICOMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dicom_dir, labels_df, masks_dir, transform=None):\n",
        "        self.dicom_dir = dicom_dir\n",
        "        self.labels_df = labels_df\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Load the DICOM image\n",
        "        subj_name = labels_df.iloc[idx]['SOPInstanceUID']\n",
        "\n",
        "        filename = subj_name + '.dcm'\n",
        "        dicom_path = os.path.join(self.dicom_dir, filename)\n",
        "        ds = pydicom.dcmread(dicom_path)\n",
        "\n",
        "        # Get the label\n",
        "        label = labels_df.iloc[idx]['Label']\n",
        "\n",
        "        # Convert to PIL image\n",
        "        img_np = np.float32(ds.pixel_array)\n",
        "\n",
        "\n",
        "        # applying mask on image numpy array before converting that to torch tensor\n",
        "        if label == 1 :\n",
        "            mask_name =  subj_name + '.png'\n",
        "            mask_path = os.path.join(masks_dir, mask_name)\n",
        "            mask = cv2.imread(mask_path, cv2.COLOR_BGR2GRAY)\n",
        "            mask = np.array(mask)\n",
        "            # print(type(img_np))\n",
        "            img_np = mask*img_np\n",
        "\n",
        "        img = Image.fromarray(img_np).convert('L')\n",
        "        # img = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def set_train(self, train=True):\n",
        "        self.train = train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weZ4jLrIyt73"
      },
      "outputs": [],
      "source": [
        "def bi_cls_stratified_split (dataset: torch.utils.data.Dataset, fraction, random_state=None) :\n",
        "    if random_state : random.seed(random_state)\n",
        "\n",
        "    indices_per_label = defaultdict(list)\n",
        "    for index, (image, label) in enumerate (dataset) :\n",
        "        indices_per_label[label].append(index)\n",
        "\n",
        "    train_indices, val_indices = list(), list()\n",
        "    for label, indices in indices_per_label.items():\n",
        "        n_samples_for_label = round(len(indices) * fraction)\n",
        "        random_indices_sample = random.sample(indices, n_samples_for_label)\n",
        "        train_indices.extend(random_indices_sample)\n",
        "        val_indices.extend(set(indices) - set(random_indices_sample))\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAc26xW46qLK"
      },
      "outputs": [],
      "source": [
        "dataset = DICOMDataset(dicom_dir, labels_df, masks_dir, transform)\n",
        "trainset, validationset = bi_cls_stratified_split(dataset, 0.8)\n",
        "train_loader = DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
        "test_loader = DataLoader(validationset, batch_size = batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgF_rGkAlfbU",
        "outputId": "58902824-b55e-4255-d086-890953ef6c32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "717"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwx2PgPQ6foz"
      },
      "source": [
        "# stratified split with separated labels array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hl6nVCvcJUY"
      },
      "outputs": [],
      "source": [
        "def stratified_split(dataset : torch.utils.data.Dataset, labels, fraction, random_state=None):\n",
        "    if random_state: random.seed(random_state)\n",
        "    indices_per_label = defaultdict(list)\n",
        "    for index, label in enumerate(labels):\n",
        "        indices_per_label[label].append(index)\n",
        "    first_set_indices, second_set_indices = list(), list()\n",
        "    for label, indices in indices_per_label.items():\n",
        "        n_samples_for_label = round(len(indices) * fraction)\n",
        "        random_indices_sample = random.sample(indices, n_samples_for_label)\n",
        "        first_set_indices.extend(random_indices_sample)\n",
        "        second_set_indices.extend(set(indices) - set(random_indices_sample))\n",
        "    first_set_inputs = torch.utils.data.Subset(dataset, first_set_indices)\n",
        "    first_set_labels = list(map(labels.__getitem__, first_set_indices))\n",
        "    second_set_inputs = torch.utils.data.Subset(dataset, second_set_indices)\n",
        "    second_set_labels = list(map(labels.__getitem__, second_set_indices))\n",
        "    return first_set_inputs, first_set_labels, second_set_inputs, second_set_labels\n",
        "\n",
        "def fun(bs) :\n",
        "    indices = np.arange(0, len(dataset))\n",
        "    train_dl = DataLoader(dataset, bs, sampler=torch.utils.data.SubsetRandomSampler(indices[:300]))\n",
        "    test_dl  = DataLoader(dataset, bs, sampler=torch.utils.data.SubsetRandomSampler(indices[-300:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDXljfwOcb4H",
        "outputId": "8481d572-53b8-41db-8850-8767589de7ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqyX_YqwTrCu"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaLIZy0lTstO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8GVb4zYWeY4"
      },
      "outputs": [],
      "source": [
        "class Encoder1 (nn.Module) :\n",
        "  def __init__ (self,\n",
        "                in_width : int,\n",
        "                in_height : int,\n",
        "                num_input_channels : int,\n",
        "                base_channel_size : int,\n",
        "                latent_dim : int,\n",
        "                act_fn : object = nn.LeakyReLU) :\n",
        "\n",
        "    self.in_height : int = in_height\n",
        "    self.in_width : int = in_width\n",
        "    super().__init__()\n",
        "    c_hid = base_channel_size\n",
        "    flat_len : int = int (2*(in_height/32)*(in_width/32)*c_hid)\n",
        "    # flat_len = 2*64*c_hid\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # h*w => h/2 * w/2\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/2 * w/2 => h/4 * w/4\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/4 * w/4 => h/8 * w/8\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/8 * w/8 => h/16 * w/16\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/16 * w/16 => h/32 * w/32\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(flat_len, latent_dim),\n",
        "        nn.BatchNorm1d(latent_dim),\n",
        "        act_fn(),\n",
        "        nn.Linear(latent_dim, 16),\n",
        "        nn.BatchNorm1d(16),\n",
        "        act_fn(),\n",
        "        # nn.Linear(16, 8),\n",
        "        # nn.BatchNorm1d(8),\n",
        "        # act_fn(),\n",
        "        nn.Linear(16, 4),\n",
        "        nn.BatchNorm1d(4),\n",
        "        act_fn()\n",
        "    )\n",
        "\n",
        "  def forward (self, x) :\n",
        "    conv_out = self.conv(x)\n",
        "    # print(conv_out.shape)\n",
        "    # flat = torch.Tensor.flatten(conv_out)\n",
        "    # print(flat.shape)\n",
        "    return self.fc(conv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYxpXTeOutBz"
      },
      "outputs": [],
      "source": [
        "class Decoder1 (nn.Module) :\n",
        "  def __init__ (self,\n",
        "                in_width : int,\n",
        "                in_height : int,\n",
        "                num_input_channels : int,\n",
        "                base_channel_size : int,\n",
        "                latent_dim : int,\n",
        "                act_fn : object = nn.LeakyReLU) :\n",
        "\n",
        "    self.in_height = in_height\n",
        "    self.in_width = in_width\n",
        "    super().__init__()\n",
        "    c_hid = base_channel_size\n",
        "    flat_len : int = int (2*(in_height/32)*(in_width/32)*c_hid)\n",
        "    # flat_len = 2*16*c_hid\n",
        "\n",
        "\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        # nn.Linear(4, 8),\n",
        "        # nn.BatchNorm1d(8),\n",
        "        # act_fn(),\n",
        "        nn.Linear(4, 16),\n",
        "        nn.BatchNorm1d(16),\n",
        "        act_fn(),\n",
        "        nn.Linear(16, latent_dim),\n",
        "        nn.BatchNorm1d(latent_dim),\n",
        "        act_fn(),\n",
        "        nn.Linear(latent_dim, flat_len),\n",
        "        nn.BatchNorm1d(flat_len),\n",
        "        act_fn()\n",
        "    )\n",
        "\n",
        "    self.convT = nn.Sequential(\n",
        "        nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/32 => h/16\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/16 => h/8\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/8 => h/4\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/4 * w/4 => h/2 * w/2\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # h/2 * w/2 => h*w\n",
        "        # act_fn()\n",
        "        nn.Sigmoid() # should be used with normalized input between -1 and 1\n",
        "    )\n",
        "\n",
        "  def forward(self, x) :\n",
        "\n",
        "    flat = self.fc(x)\n",
        "    # print(flat.size())\n",
        "    convT_in = flat.reshape(flat.shape[0], -1, int(self.in_height/32), int(self.in_width/32))\n",
        "    return self.convT(convT_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d52KbWnYP_Gp"
      },
      "outputs": [],
      "source": [
        "enc = Encoder1(224, 224, 1, 32, 64)\n",
        "dec = Decoder1(224, 224, 1, 32, 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9h_gprpRj66"
      },
      "outputs": [],
      "source": [
        "class AE1 (nn.Module) :\n",
        "  def __init__(self,\n",
        "               base_channel_size: int,\n",
        "               latent_dim: int,\n",
        "            #    encoder_obj : object = Encoder,\n",
        "            #    decoder_obj : object = Decoder,\n",
        "               num_input_channels: int = 3,\n",
        "               width: int = 32,\n",
        "               height: int = 32) :\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder (width, height, num_input_channels, base_channel_size, latent_dim)\n",
        "    self.decoder = Decoder (width, height, num_input_channels, base_channel_size, latent_dim)\n",
        "\n",
        "  def forward (self, x) :\n",
        "    encoded = self.encoder(x)\n",
        "    return self.decoder(encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jh70EhdNewE"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBQ1Cow6Npgi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjO21WQGNqpk"
      },
      "outputs": [],
      "source": [
        "class Encoder (nn.Module) :\n",
        "  def __init__ (self,\n",
        "                in_width : int,\n",
        "                in_height : int,\n",
        "                num_input_channels : int,\n",
        "                base_channel_size : int,\n",
        "                latent_dim : int,\n",
        "                act_fn : object = nn.LeakyReLU) :\n",
        "\n",
        "    self.in_height : int = in_height\n",
        "    self.in_width : int = in_width\n",
        "    super().__init__()\n",
        "    c_hid = base_channel_size\n",
        "    flat_len : int = int (2*(in_height/4)*(in_width/4)*c_hid)\n",
        "    # flat_len = 2*64*c_hid\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # h*w => h/2 * w/2\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/2 * w/2 => h/4 * w/4\n",
        "        nn.BatchNorm2d(2*c_hid),\n",
        "        act_fn(),\n",
        "        # nn.Conv2d(2*c_hid, 4*c_hid, kernel_size=3, padding=1, stride=2), # h/4 * w/4 => h/8 * w/8\n",
        "        # nn.BatchNorm2d(4*c_hid),\n",
        "        # act_fn(),\n",
        "        # nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        # nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # h/8 * w/8 => h/16 * w/16\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(flat_len, latent_dim),\n",
        "        nn.BatchNorm1d(latent_dim),\n",
        "        act_fn(),\n",
        "        nn.Linear(latent_dim, 16),\n",
        "        nn.BatchNorm1d(16),\n",
        "        act_fn(),\n",
        "        nn.Linear(16, 8),\n",
        "        nn.BatchNorm1d(8),\n",
        "        act_fn(),\n",
        "        nn.Linear(8, 4),\n",
        "        nn.BatchNorm1d(4),\n",
        "        act_fn()\n",
        "    )\n",
        "\n",
        "  def forward (self, x) :\n",
        "    conv_out = self.conv(x)\n",
        "    # print(conv_out.shape)\n",
        "    # flat = torch.Tensor.flatten(conv_out)\n",
        "    # print(flat.shape)\n",
        "    return self.fc(conv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieqq52iNNqpl"
      },
      "outputs": [],
      "source": [
        "class Decoder (nn.Module) :\n",
        "  def __init__ (self,\n",
        "                in_width : int,\n",
        "                in_height : int,\n",
        "                num_input_channels : int,\n",
        "                base_channel_size : int,\n",
        "                latent_dim : int,\n",
        "                act_fn : object = nn.LeakyReLU) :\n",
        "\n",
        "    self.in_height = in_height\n",
        "    self.in_width = in_width\n",
        "    super().__init__()\n",
        "    c_hid = base_channel_size\n",
        "    flat_len : int = int (2*(in_height/4)*(in_width/4)*c_hid)\n",
        "    # flat_len = 2*16*c_hid\n",
        "\n",
        "\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(4, 8),\n",
        "        nn.BatchNorm1d(8),\n",
        "        act_fn(),\n",
        "        nn.Linear(8, 16),\n",
        "        nn.BatchNorm1d(16),\n",
        "        act_fn(),\n",
        "        nn.Linear(16, latent_dim),\n",
        "        nn.BatchNorm1d(latent_dim),\n",
        "        act_fn(),\n",
        "        nn.Linear(latent_dim, flat_len),\n",
        "        nn.BatchNorm1d(flat_len),\n",
        "        act_fn()\n",
        "    )\n",
        "\n",
        "    self.convT = nn.Sequential(\n",
        "        # nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/32 => h/16\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        # nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/16 => h/8\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        # nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        # nn.ConvTranspose2d(4*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/8 => h/4\n",
        "        # nn.BatchNorm2d(2*c_hid),\n",
        "        # act_fn(),\n",
        "        nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # h/4 * w/4 => h/2 * w/2\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(c_hid),\n",
        "        act_fn(),\n",
        "        nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # h/2 * w/2 => h*w\n",
        "        # act_fn()\n",
        "        nn.Sigmoid() # should be used with normalized input between -1 and 1\n",
        "    )\n",
        "\n",
        "  def forward(self, x) :\n",
        "\n",
        "    flat = self.fc(x)\n",
        "    # print(flat.size())\n",
        "    convT_in = flat.reshape(flat.shape[0], -1, int(self.in_height/4), int(self.in_width/4))\n",
        "    return self.convT(convT_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIuUeJkONqpm"
      },
      "outputs": [],
      "source": [
        "class AE (nn.Module) :\n",
        "  def __init__(self,\n",
        "               base_channel_size: int,\n",
        "               latent_dim: int,\n",
        "            #    encoder_obj : object = Encoder,\n",
        "            #    decoder_obj : object = Decoder,\n",
        "               num_input_channels: int = 3,\n",
        "               width: int = 32,\n",
        "               height: int = 32) :\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder (width, height, num_input_channels, base_channel_size, latent_dim)\n",
        "    self.decoder = Decoder (width, height, num_input_channels, base_channel_size, latent_dim)\n",
        "\n",
        "  def forward (self, x) :\n",
        "    encoded = self.encoder(x)\n",
        "    return self.decoder(encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpz4mp_P8dgb"
      },
      "outputs": [],
      "source": [
        "autoencoder = AE (32, 64, 1, 224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDRq6fktl5FV"
      },
      "outputs": [],
      "source": [
        "class AE_FC_classifier (nn.Module) :\n",
        "  def __init__(self,\n",
        "               Encoder : object,\n",
        "               in_features : int,\n",
        "               num_classes : int) :\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder\n",
        "    self.classifier = nn.Sequential (\n",
        "        nn.Linear(in_features, num_classes),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward (self, x) :\n",
        "    return self.classifier(self.Encoder(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv4D97AE9pJP"
      },
      "outputs": [],
      "source": [
        "class AE_train_val :\n",
        "  def __init__ (self, model, n_epochs, criterion,\n",
        "                optimizer, learning_rate,\n",
        "                lr_scheduler = None) :\n",
        "\n",
        "    self.model = model\n",
        "    self.criterion = criterion\n",
        "    self.optimizer = optimizer\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_epochs = n_epochs\n",
        "    self.lr_sch = lr_scheduler\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.val_losses = []\n",
        "    self.train_losses = []\n",
        "\n",
        "  def train(self, AE_train_loader, AE_val_loader) :\n",
        "    for epoch in range(self.n_epochs) :\n",
        "\n",
        "      running_train_loss = 0.0\n",
        "      running_val_loss = 0.0\n",
        "\n",
        "      self.model.train()\n",
        "      avg_train_loss = self.train_epoch(AE_train_loader, running_train_loss)\n",
        "      self.model.eval()\n",
        "      avg_val_loss = self.val_epoch(AE_val_loader, running_val_loss)\n",
        "\n",
        "      print ('epoch {}/{}: avg_train_loss: {} \\t avg_val_loss: {}'.format(epoch, self.n_epochs, avg_train_loss, avg_val_loss))\n",
        "\n",
        "\n",
        "\n",
        "  def train_epoch (self, AE_train_loader, running_train_loss) :\n",
        "    for batch, labels in AE_train_loader :\n",
        "      batch = batch.float().to(self.device)\n",
        "    #   print (batch.shape)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      outputs = self.model(batch)\n",
        "    #   print(batch.size(), outputs.size())\n",
        "\n",
        "      loss = self.criterion (outputs, batch)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      running_train_loss += loss.item()\n",
        "\n",
        "\n",
        "    # train stats\n",
        "    avg_train_loss = running_train_loss / len(AE_train_loader.dataset)\n",
        "    self.train_losses.append (avg_train_loss)\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "  def val_epoch (self, AE_val_loader, running_val_loss) :\n",
        "    with torch.no_grad() :\n",
        "      for batch, labels in AE_val_loader :\n",
        "        batch = batch.to(self.device).float()\n",
        "        outputs = self.model(batch)\n",
        "\n",
        "        loss = self.criterion (outputs, batch)\n",
        "\n",
        "        running_val_loss += loss.item()\n",
        "\n",
        "      # validation stats\n",
        "      avg_val_loss = running_val_loss / len(AE_val_loader.dataset)\n",
        "      self.val_losses.append (avg_val_loss)\n",
        "\n",
        "      return avg_val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlVPpEkacHFn",
        "outputId": "048e537a-4232-412a-b537-2bd312dfb76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0/10: avg_train_loss: 0.12760715871243863 \t avg_val_loss: 0.1439437807775011\n",
            "epoch 1/10: avg_train_loss: 0.12457956318773275 \t avg_val_loss: 0.14331549756667195\n",
            "epoch 2/10: avg_train_loss: 0.12257506337739911 \t avg_val_loss: 0.1435542071566862\n",
            "epoch 3/10: avg_train_loss: 0.12176948536818971 \t avg_val_loss: 0.14321657021840414\n",
            "epoch 4/10: avg_train_loss: 0.12356957961651852 \t avg_val_loss: 0.1432087877217461\n",
            "epoch 5/10: avg_train_loss: 0.12317615352040134 \t avg_val_loss: 0.14321302082024367\n",
            "epoch 6/10: avg_train_loss: 0.12223558197443257 \t avg_val_loss: 0.14321964745428048\n",
            "epoch 7/10: avg_train_loss: 0.1211758787567551 \t avg_val_loss: 0.14320906587675505\n",
            "epoch 8/10: avg_train_loss: 0.12174674949130497 \t avg_val_loss: 0.1432080210423937\n",
            "epoch 9/10: avg_train_loss: 0.12089975723763356 \t avg_val_loss: 0.14320450086219638\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "\n",
        "AE_trainer = AE_train_val(autoencoder, 10, criterion, optimizer, 0.01)\n",
        "# print(torch.shape(train_loader))\n",
        "AE_trainer.train(train_loader, test_loader)\n",
        "# batch , label = next(iter(train_loader))\n",
        "# out = autoencoder(batch)\n",
        "# out.shape, batch.shape\n",
        "# type(torch.Tensor.to_numpy(out))\n",
        "# oo = out.detach().numpy()\n",
        "# oo[0].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "UkgO0mFb8ka8",
        "outputId": "19b35900-70db-45e4-d8ec-605f0e1f9937"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pl1 = plt.plot(AE_trainer.train_losses, label = 'train')\n",
        "pl2 = plt.plot(AE_trainer.val_losses, label = 'validation')\n",
        "leg = plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nvu2N9sfD-f"
      },
      "source": [
        "# ConvAutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpogyA2Awuvo"
      },
      "outputs": [],
      "source": [
        "class AE_cls_train_val :\n",
        "  def __init__ (self, model, n_epochs, criterion_AE, criterion_cls,\n",
        "                optimizer,\n",
        "                lr_scheduler = None) :\n",
        "\n",
        "    self.model = model\n",
        "    self.criterion_AE = criterion_AE\n",
        "    self.criterion_cls = criterion_cls\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.n_epochs = n_epochs\n",
        "    self.lr_sch = lr_scheduler\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.val_losses = []\n",
        "    self.val_accs = []\n",
        "    self.val_f1s = []\n",
        "    self.train_losses = []\n",
        "    self.train_accs = []\n",
        "    self.train_f1s = []\n",
        "\n",
        "\n",
        "    # self.val_losses_cls = []\n",
        "    # self.val_losses_rec = []\n",
        "    # self.train_losses_cls = []\n",
        "    # self.train_losses_rec = []\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, AE_train_loader, AE_val_loader) :\n",
        "    for epoch in range(self.n_epochs) :\n",
        "      train_preds = []\n",
        "      val_preds = []\n",
        "      train_labels = []\n",
        "      val_labels = []\n",
        "\n",
        "      self.model.train()\n",
        "      avg_train_loss, train_acc, train_f1 = self.train_epoch(AE_train_loader, train_preds, train_labels)\n",
        "      self.model.eval()\n",
        "      avg_val_loss, val_acc, val_f1 = self.val_epoch(AE_val_loader, val_preds, val_labels)\n",
        "\n",
        "      print ('[epoch %d/%d] avg_train_loss: %.5f \\t avg_val_loss: %.5f \\t train_f1: %.15f \\t val_f1: %.15f' % (epoch+1, self.n_epochs,\n",
        "                                                                                                             avg_train_loss, avg_val_loss,\n",
        "                                                                                                             train_f1, val_f1))\n",
        "\n",
        "\n",
        "\n",
        "  def train_epoch (self, AE_train_loader, train_preds, train_labels, cl) :\n",
        "    running_train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    for batch, labels in AE_train_loader :\n",
        "      batch = batch.float().to(self.device)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      reconstructeds, probs = self.model(batch)\n",
        "\n",
        "      loss_rec = self.criterion_AE (reconstructeds, batch)\n",
        "      loss_cls = self.criterion_cls (probs, labels)\n",
        "      loss = loss_rec + loss_cls\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      _, predicted = torch.max(classes, 1)\n",
        "      train_correct += (predicted == labels).sum().item()\n",
        "      running_train_loss += loss.item()\n",
        "      train_preds.append(predicted)\n",
        "      train_labels.append(labels)\n",
        "\n",
        "    # train stats\n",
        "    train_preds = np.concatenate(train_preds)\n",
        "    train_labels = np.concatenate(train_labels)\n",
        "    _, train_f1_score, _, _ = precision_recall_fscore_support(train_labels, train_preds, average= 'weighted')\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(AE_train_loader.dataset)\n",
        "    train_acc = train_correct / len(AE_train_loader.dataset)\n",
        "\n",
        "    self.train_losses.append (avg_train_loss)\n",
        "    self.train_accs.append (train_acc)\n",
        "    self.train_f1s.append (train_f1_score)\n",
        "\n",
        "    return avg_train_loss, train_acc, train_f1_score\n",
        "\n",
        "  def val_epoch (self, AE_val_loader, val_preds, val_labels) :\n",
        "    running_val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    with torch.no_grad() :\n",
        "      for batch, labels in AE_val_loader :\n",
        "        batch = batch.to(self.device).float()\n",
        "        reconstructeds, classes = self.model(batch)\n",
        "        loss_rec = self.criterion_AE (reconstructeds, batch)\n",
        "        loss_cls = self.criterion_cls (classes, labels)\n",
        "        loss = loss_rec + loss_cls\n",
        "\n",
        "        _, predicted = torch.max(classes, 1)\n",
        "        val_correct += (predicted == labels).sum().item()\n",
        "        running_val_loss += loss.item()\n",
        "        val_preds.append(predicted)\n",
        "        val_labels.append(labels)\n",
        "\n",
        "      # validation stats\n",
        "      val_preds = np.concatenate(val_preds)\n",
        "      val_labels = np.concatenate(val_labels)\n",
        "      _, val_f1_score, _, _ = precision_recall_fscore_support(val_labels, val_preds, average= 'weighted')\n",
        "    #   _, val_f1_mic, _, _ = precision_recall_fscore_support(val_labels, val_preds, average= 'micro')\n",
        "\n",
        "\n",
        "      avg_val_loss = running_val_loss / len(AE_val_loader.dataset)\n",
        "      val_acc = val_correct / len(AE_val_loader.dataset)\n",
        "\n",
        "      self.val_losses.append (avg_val_loss)\n",
        "      self.val_accs.append (val_acc)\n",
        "      self.val_f1s.append (val_f1_score)\n",
        "\n",
        "      return avg_val_loss, val_acc, val_f1_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ST7wfeTfDTz",
        "outputId": "1cf46e34-5bdd-4e5d-b868-23fdd7a4bbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1/10] avg_train_loss: 0.14301 \t avg_val_loss: 0.12272 \t train_f1: 0.965132496513250 \t val_f1: 1.000000000000000\n",
            "[epoch 2/10] avg_train_loss: 0.11343 \t avg_val_loss: 0.09734 \t train_f1: 0.974895397489540 \t val_f1: 1.000000000000000\n",
            "[epoch 3/10] avg_train_loss: 0.09503 \t avg_val_loss: 0.08291 \t train_f1: 0.986052998605300 \t val_f1: 1.000000000000000\n",
            "[epoch 4/10] avg_train_loss: 0.08206 \t avg_val_loss: 0.07133 \t train_f1: 1.000000000000000 \t val_f1: 1.000000000000000\n",
            "[epoch 5/10] avg_train_loss: 0.07334 \t avg_val_loss: 0.06437 \t train_f1: 1.000000000000000 \t val_f1: 1.000000000000000\n",
            "[epoch 6/10] avg_train_loss: 0.06920 \t avg_val_loss: 0.06214 \t train_f1: 0.998605299860530 \t val_f1: 1.000000000000000\n",
            "[epoch 7/10] avg_train_loss: 0.06530 \t avg_val_loss: 0.06025 \t train_f1: 0.998605299860530 \t val_f1: 1.000000000000000\n",
            "[epoch 8/10] avg_train_loss: 0.06146 \t avg_val_loss: 0.05833 \t train_f1: 1.000000000000000 \t val_f1: 1.000000000000000\n",
            "[epoch 9/10] avg_train_loss: 0.05990 \t avg_val_loss: 0.05766 \t train_f1: 0.998605299860530 \t val_f1: 1.000000000000000\n",
            "[epoch 10/10] avg_train_loss: 0.06050 \t avg_val_loss: 0.05661 \t train_f1: 0.998605299860530 \t val_f1: 1.000000000000000\n"
          ]
        }
      ],
      "source": [
        "ae_cls = ConvAutoencoder()\n",
        "criterion_AE = nn.MSELoss()\n",
        "\n",
        "class_weights = class_weight.compute_\n",
        "criterion_cls = nn.NLLLoss()\n",
        "optimizer = optim.Adam(ae_cls.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "\n",
        "AE_cls_trainer = AE_cls_train_val(ae_cls, 10, criterion_AE, criterion_cls, optimizer)\n",
        "# print(torch.shape(train_loader))\n",
        "AE_cls_trainer.train(train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlluPAjl3mNe",
        "outputId": "764cd6a3-3a72-4802-a218-0baac4c8a232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1/10] avg_train_loss: 0.17738 \t avg_val_loss: 0.17784 \t train_f1: 0.730723606168446 \t val_f1: 0.845788849347568\n",
            "[epoch 2/10] avg_train_loss: 0.17760 \t avg_val_loss: 0.17703 \t train_f1: 0.708185053380783 \t val_f1: 0.880189798339265\n",
            "[epoch 3/10] avg_train_loss: 0.17750 \t avg_val_loss: 0.17683 \t train_f1: 0.731909845788849 \t val_f1: 0.876631079478055\n",
            "[epoch 4/10] avg_train_loss: 0.17762 \t avg_val_loss: 0.17726 \t train_f1: 0.750889679715303 \t val_f1: 0.874258600237248\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-bd995534ac82>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfinal_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE_cls_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_AE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-6483e2765ed0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, AE_train_loader, AE_val_loader)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mavg_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-6483e2765ed0>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, AE_train_loader, train_preds, train_labels)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mrunning_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAE_train_loader\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m       \u001b[0;31m# print(type(batch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# print(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-096ec6175c40>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Convert to PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# applying mask on image numpy array before converting that to torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "final_loader = DataLoader(dataset, batch_size= batch_size, shuffle=True)\n",
        "final_model = ConvAutoencoder()\n",
        "final_trainer = AE_cls_train_val(final_model, 10, criterion_AE, criterion_cls, optimizer)\n",
        "final_trainer.train(final_loader, final_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCfH3J0Y2MHz",
        "outputId": "415f04fe-942b-45cb-a7c6-8e6b9abc00e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=128, out_features=2, bias=True)\n",
              "  (1): Softmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_model.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4dE8FkLwVU1"
      },
      "outputs": [],
      "source": [
        "class FinalClassifier (nn.Module) :\n",
        "    def __init__(self, ae_model : ConvAutoencoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc = ae_model.enc\n",
        "        self.classifier = ae_model.classifier\n",
        "\n",
        "    def forward(self, x) :\n",
        "        z = self.enc(x)\n",
        "        out = self.classifier(z)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "-V8VAbT92Y4w",
        "outputId": "ed98b30a-e9e9-4d03-c51d-bd4aa48284c2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e329d93ed9d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinalClassifier\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
          ]
        }
      ],
      "source": [
        "final_classifier = FinalClassifier (final_model)\n",
        "final_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vffIjCo2geW"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/DataWizardS/content/iaaa-data/models/DataWizardS_Fourth_Model_AE_Classifier.pth'\n",
        "\n",
        "torch.save(final_classifier.state_dict(), path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dX_NrzNOpk-"
      },
      "source": [
        "# Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z9KJjcxrx31"
      },
      "outputs": [],
      "source": [
        "class TestDICOMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        # self.labels_df = labels_df\n",
        "        # self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        self.dicom_paths = os.listdir(self.dicom_dir)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dicom_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Load the DICOM image\n",
        "        # subj_name = labels_df.iloc[idx]['SOPInstanceUID']\n",
        "\n",
        "        # filename = subj_name + '.dcm'\n",
        "        # dicom_path = os.path.join(self.dicom_dir, filename)\n",
        "\n",
        "        dicom_path = dicom_path[idx]\n",
        "        filename = dicom_path.split('/')[-1]\n",
        "        ds = pydicom.dcmread(dicom_path)\n",
        "\n",
        "        # Get the label\n",
        "        # label = labels_df.iloc[idx]['Label']\n",
        "\n",
        "        # Convert to PIL image\n",
        "        img_np = np.float64(ds.pixel_array)\n",
        "\n",
        "        # applying mask on image numpy array before converting that to torch tensor\n",
        "        # if label == 0 :\n",
        "        #     mask_name =  subj_name + '.png'\n",
        "        #     mask_path = os.path.join(masks_dir, mask_name)\n",
        "        #     mask = cv2.imread(mask_path, cv2.COLOR_BGR2GRAY)\n",
        "        #     img_np = mask*img_np  # same as using cv2.bitwise_and()\n",
        "\n",
        "            # img_np = cv2.bitwise_and(img_np, img_np, mask = mask)\n",
        "\n",
        "        img = Image.fromarray(img_np).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return (filename, img), label\n",
        "\n",
        "\n",
        "\n",
        "# labels_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNyMfnIr6-Wb"
      },
      "outputs": [],
      "source": [
        "class Tester :\n",
        "    def __init__(self, model) :\n",
        "        self.model = model\n",
        "\n",
        "    def test (self, dataloader, output_path) :\n",
        "        self.model.eval()\n",
        "        all_names = np.asarray([], dtype=int)\n",
        "        all_labels = np.asarray([], dtype=int)\n",
        "\n",
        "        with torch.no_grad() :\n",
        "            for name_im, _ in dataloader :\n",
        "\n",
        "                names, batch = name_im\n",
        "\n",
        "                output = self.model(batch)\n",
        "                labels = torch.argmax(output, dim=1)\n",
        "\n",
        "                names = np.asarray(names)\n",
        "                labels = labels.numpy().astype('int')\n",
        "\n",
        "                all_names = np.concatenate((all_names, names), axis=0)\n",
        "                all_labels = np.concatenate((all_labels, labels), axis=0)\n",
        "\n",
        "\n",
        "        df = pd.DataFrame({'name': all_names, 'labels': all_labels})\n",
        "        df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ59gOKPYlHi"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "\n",
        "mod = ConvAutoencoder()\n",
        "fin_mod = FinalClassifier(mod)\n",
        "\n",
        "# tmpset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# tmploader = torch.utils.data.DataLoader(tmpset, batch_size=64,\n",
        "#                                          shuffle=False, num_workers=2)\n",
        "tmpset = DICDataset(dicom_dir, labels_df, masks_dir, transform)\n",
        "\n",
        "percent = 0.1\n",
        "\n",
        "# Calculate the number of samples to include based on the percentage\n",
        "num_samples = int(percent * len(tmpset))\n",
        "\n",
        "# Randomly sample the indices of the subset\n",
        "indices = torch.randperm(len(tmpset), dtype=int)[:num_samples]\n",
        "\n",
        "tmploader = torch.utils.data.DataLoader(tmpset, batch_size=16, shuffle=False,\n",
        "                                        # sampler=SubsetRandomSampler(indices)\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvsBLhjNa-If"
      },
      "outputs": [],
      "source": [
        "t = Tester(fin_mod)\n",
        "t.test(tmploader, 'csv_path')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPTTFqMC4uWF"
      },
      "source": [
        "# CSV concatenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSnldaPP4wjU"
      },
      "outputs": [],
      "source": [
        "def csv_concat (path1, path2, output_path) :\n",
        "\n",
        "    df1 = pd.read_csv(path1)\n",
        "\n",
        "    df2 = pd.read_csv(path2)\n",
        "\n",
        "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "    combined_df.to_csv(output_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l3qBWnjaE9KF",
        "Rwx2PgPQ6foz",
        "aqyX_YqwTrCu",
        "5Jh70EhdNewE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}